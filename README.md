# lab 1

Задания:
Установите библиотеку PyTorch, следуя следующей инструкции: https://pytorch.org/get-started/locally/
Познакомтесь с "пулом" моделей PyTorch: https://pytorch.org/vision/stable/models.html
Просмотрите документацию по трассировке моделей PyTorch: https://pytorch.org/docs/stable/generated/torch.jit.trace.html
Просмотрите документацию по кодированию моделей PyTorch с помощью TorchScript: https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html
Оттрасируйте любую модель из пула torchvision
Сохраните оттрасированную модель в виде файла
Загрузите сохранённую модель, провалидируйте её на корректную конвертацию

# lab 2

Задания:
Установите библиотеку timm, следуя следующей инструкции: https://github.com/huggingface/pytorch-image-models
Познакомтесь с "пулом" моделей timm: https://timm.fast.ai/
Создайте любую модель из timm.
Напишите код запуска модели на пользовательском изображении
Найдите в интернете список с названиями классов, на которых обучалась модель и реализуйте преобразование индекса предсказанного класса в его название
Оттрасируйте с помощью torch.jit.trace модель и сохраните её
Установите библиотеку SMP
Ознакомьтесь с документацией проекта: https://smp.readthedocs.io/en/latest/
Создайте модель с помощью SMP с encoder частью из timm (список поддерживаемых timm моделей)
Попробуйте создать, загрузив веса предобученной модели на двух разных наборах данных (требуется создать две разных модели для этого), список доступных весов для разных наборов данных
Сохраните веса (state_dict) encoder части сегментационной модели в отдельный файл
Загрузите веса сохраненной модели через экземпляр класса модели, созданной с помощью timm

# lab 3

Выберите любую модель классификации из зоопарка моделей на PyTorch.
Экспортируйте модель в ONNX с помощью метода torch.onnx.export.
Попробуйте запустить модели на pytorch и на onnx (с помощью onnxruntime) на одних входных данных и сравните результаты.
Произведите конвертацию в фреймворк CoreML модели из PyTorch по следующей инструкции. Также попробуйте экспортировать модель на ONNX, полученную в предыдущем пункте в CoreML через следующую библиотеку.
(Опционально) Если у вас имеется компьютер на MacOS попробуйте запустить модели на CoreML, ONNX и PyTorch и сравните результаты.

# lab 4

Задания:
Открыть подготовленную среду выполнения Google Colab.
Следуя инструкциям в Colab запускать код Python.
Выполнить три задания, представленных в Colab.
Замечание: в случае проблем с открытием среды Google Colab к данной работе прикладывается файл NNMA_YOLO_Lab.ipynb для работы с локальной средой выполнения Jupyter Notebook.

# lab 5

Выберите любую модель классификации из зоопарка моделей на PyTorch.
Конвертируйте её по следующей инструкции.
Произведите предсказание полученной моделью на ONNX с помощью InferenceSession. Ознакомиться с запуском моделей можно тут.
Конвертируйте модель на ONNX в Tensorflow-Lite. Конвертацию модели следует производить по следующей инструкции. Произведите предсказание полученной моделью на  Tensorflow-Lite с помощью tf.lite.Interpreter. Ознакомиться с запуском моделей можно тут или по инструкции из шага конвертации (предыдущая ссылка).
Произведите шаг 4 с вашей обученной моделью YOLOv8. В библиотеке ultralytics содержится модуль export для конвертации моделей. 

# lab 6

Задания:
Установите приложение для разметки данных LabelImg из следующего репозитория https://github.com/HumanSignal/labelImg . 
При разметки данных создайте две директории: images и labels
В приложении LabelImg выберите папку с изображениями и с помощью Change Save Dir выберите директорию labels
Затем разметьте необходимое количество примеров
После этого необходимо убедится, что изображения, хранятся в директории images, а файлы разметки .txt в директории labels. Разделить ваш датасет на обучающую и валидационную выборки, причём иерархия директорий должна быть следующей:
train/
--images/
--labels/

val/
--images/
--labels/

Далее по следующей инструкции запустите обучение (без подключения ClearML) https://habr.com/ru/articles/714232/  . Есть две опции:
Воспользоваться Google Colab для обучения (ссылку можно найти в конце инструкции в разделе Environments)
Запустить обучения на локальной машине
После обучения в папке runs будут содержаться ваши эксперименты. Ориентировочно по пути runs/train/exp1/weights будут лежать файлы обученных моделей best.pt и last.pt. Необходимо загрузить лучшую модель и провести с ней шаги из предыдущей лабораторной работы

# lab 7

Предлагается выполнять данную лабораторную работу в группах из 1-3 человек. Каждая группа, выполняющая лабораторную работу выбирает свой вариант. В соответствии с выбранным вариантом необходимо будет работать с определенными видами объектов. Также проект необходимо вести в открытом GitHub репозитории (при желании можно воспользоваться сервисом GitLab).

1. Необходимо взять за основу готовую реализацию приложения для Android/iOS с задачей детекции объектов (в лекции 3 можно найти ссылки на соответсвующие репозитории)

2. По инструкции, описанной в лабораторной работе 2 или 4 обучить собственную модель YOLOv8. Классы выбираются в соответствии с выбранным вариантом. Списки наименований классов представлены ниже в описании лабораторной работы. При желании можно воспользоваться другими архитектурами для решения задачи детекции объектов, например YoloX или YOLOv7.

3. Далее необходимо экспортировать модель в целевой фреймворк. Для iOS устройств следует выбирать CoreML, для Android можно воспользоваться либо Tensorflow-Lite, либо PyTorch Mobile. Экспортировать модель следует скриптом export.py по следующей инструкции.

4. Собрать решение и запустить на мобильном устройстве данную модель, оценить скорость работы архитектуры (добавить профилирование времени работы вызова функции с моделью). Для Android можно использовать следующий репозиторий или фреймворк PlayTorch, ссылка на реализацию с object detection (рекомендуется этот подход).

5. Необходимо продемонстрировать работу модели на тестовых изображениях. Тестовый набор данных с разметкой будет предоставляться при сдачи задания группой. Для успешной сдачи на тестовой выборке модель должна достигать значений метрики mAP@0.5 не ниже 0.8. Значения метрики могут быть скорректированы позднее.

Наборы классов для каждого варианта:

1. зубная щетка, тюбик зубной пасты, мыло (твердое, при желании можно добавить 4-й класс с тарой жидкого мыла)
2. кабачок, помидор, огурец
3. денежная купюра, монета, банковская карта
4. классическая гитара, акустическая гитара, электрогитара (при желании можно добавить 4-й класс с полуакустической гитарой)
5. стул, стол, кровать, диван

Требования к репозиторию и проводимым экспериментам:

Проводимые эксперименты должны быть воспроизводимы, следовательно следует прикрепить ссылку на размеченный набор данных
Репозиторий YOLOv8 (если выбрана другая реализация, то её репоизиторий соответственно) необходимо добавить к репозиторию в качестве submodule, про это можно почитать в данном описании.
Код для обучения должен запускаться из Docker контейнера, поэтому необходимо обернуть проведение экспериментов в Docker образ. При создании контейнера для экспериментов передаются следующие параметры: директория с размеченным набором данных (разбиение на тренировочную и валидационную части можно произвести заранее), директория с результатами обучения и логированием эксперимента (в неё можно скопировать всё содержимое папки runs/train/exp*/) Ознакомиться с созданием Docker образов можно по следующей ссылке.
При сильном желании можно перейти от задачи детекции к задаче выделения объектов масками (instance segmentation). Классическим приложением для разметки может выступать инструмент LabelMe, или же полуавтоматизированные инструменты, например как RITM. Также в первой ссылке этого абзаца содержится описание альтернативного инструмента для полуавтоматической разметки, также можно воспользоваться им. Соответственно пример кода приложения на iOS/Android для внедрения также нужно будет выбирать другой, ориентированный на решение instance segmentation.
